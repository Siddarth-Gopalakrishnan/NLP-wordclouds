{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275d9e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataPrep\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import RSLPStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "import joblib\n",
    "\n",
    "# Modeling\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#from sklearn.linear_model import LogisticRegression\n",
    "#from sklearn.naive_bayes import GaussianNB\n",
    "#import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bade9462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from matplotlib.gridspec import GridSpec\n",
    "pd.set_option('display.max_columns', 100)\n",
    "import plotly.offline as py\n",
    "import plotly.express as px\n",
    "import plotly.graph_objs as go\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17f6867",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"BCA_Airbnb_reviews .csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adab895",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comments = df.loc[:, ['Reviews']]\n",
    "#df_comments = df.loc[:, ['Reviews','score']] # score = ratings column (if u have ratings also in data )\n",
    "#df_comments = df_comments.dropna(subset=['Review Description'])\n",
    "df_comments = df_comments.reset_index(drop=True)\n",
    "print(f'Dataset shape: {df_comments.shape}')\n",
    "df_comments.columns = ['comment']\n",
    "#df_comments.columns = ['comment','score']\n",
    "df_comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548ae6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comments.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ff4582",
   "metadata": {},
   "source": [
    "# Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e97066",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_patterns(re_pattern, text_list):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    ---------\n",
    "    re_pattern: regular expression pattern to be used on search [type: string]\n",
    "    text_list: list with text strings [type: list]\n",
    "    \n",
    "    Returns:\n",
    "    positions_dict: python dictionary with key-value pars as below:\n",
    "        text_idx: [(start_pattern1, end_pattern1), (start_pattern1, end_pattern2), ... (start_n, end_n)]\n",
    "    \"\"\"\n",
    "    \n",
    "    # Compiling the Regular Expression passed as a arg\n",
    "    p = re.compile(re_pattern)\n",
    "    positions_dict = {}\n",
    "    i = 0\n",
    "    for c in text_list:\n",
    "        match_list = []\n",
    "        iterator = p.finditer(c)\n",
    "        for match in iterator:\n",
    "            match_list.append(match.span())\n",
    "        control_key = f'Text idx {i}'\n",
    "        if len(match_list) == 0:\n",
    "            pass\n",
    "        else:\n",
    "            positions_dict[control_key] = match_list\n",
    "        i += 1\n",
    "        \n",
    "    \"\"\"p = '[R]{0,1}\\$[ ]{0,}\\d+(,|\\.)\\d+'\n",
    "    pattern_dict = find_patterns(p, reviews_breakline)\n",
    "    print(len(pattern_dict))\n",
    "    pattern_dict\n",
    "    for idx in [int(c.split(' ')[-1]) for c in list(pattern_dict.keys())]:\n",
    "        print(f'{reviews_breakline[idx]}\\n')\"\"\"\n",
    "\n",
    "    return positions_dict\n",
    "\n",
    "def print_step_result(text_list_before, text_list_after, idx_list):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    ----------\n",
    "    text_list_before: list object with text content before transformation [type: list]\n",
    "    text_list_after: list object with text content after transformation [type: list]\n",
    "    idx_list: list object with indexes to be printed [type: list]\n",
    "    \"\"\"\n",
    "    \n",
    "    # Iterating over string examples\n",
    "    i = 1\n",
    "    for idx in idx_list:\n",
    "        print(f'--- Text {i} ---\\n')\n",
    "        print(f'Before: \\n{text_list_before[idx]}\\n')\n",
    "        print(f'After: \\n{text_list_after[idx]}\\n')\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99053904",
   "metadata": {},
   "outputs": [],
   "source": [
    "def re_breakline(text_list):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    ----------\n",
    "    text_list: list object with text content to be prepared [type: list]\n",
    "    \"\"\"\n",
    "    \n",
    "    # Applying regex\n",
    "    return [re.sub('[\\n\\r]', ' ',str(r)) for r in text_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1d104a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list of comment reviews\n",
    "reviews = list(df_comments['comment'].values)\n",
    "\n",
    "# Applying RegEx\n",
    "reviews_breakline = re_breakline(reviews)\n",
    "df_comments['re_breakline'] = reviews_breakline\n",
    "\n",
    "# Verifying results\n",
    "print_step_result(reviews, reviews_breakline, idx_list=[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b021afa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def re_hiperlinks(text_list):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    ----------\n",
    "    text_list: list object with text content to be prepared [type: list]\n",
    "    \"\"\"\n",
    "    \n",
    "    # Applying regex\n",
    "    pattern = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "    return [re.sub(pattern, ' link ', r) for r in text_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbb78e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying RegEx\n",
    "reviews_hiperlinks = re_hiperlinks(reviews_breakline)\n",
    "df_comments['re_hiperlinks'] = reviews_hiperlinks\n",
    "\n",
    "# Verifying results\n",
    "print_step_result(reviews_breakline, reviews_hiperlinks, idx_list=[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3b001b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be68eb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def re_dates(text_list):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    ----------\n",
    "    text_list: list object with text content to be prepared [type: list]\n",
    "    \"\"\"\n",
    "    \n",
    "    # Applying regex\n",
    "    pattern = '([0-2][0-9]|(3)[0-1])(\\/|\\.)(((0)[0-9])|((1)[0-2]))(\\/|\\.)\\d{2,4}'\n",
    "    return [re.sub(pattern, ' data ', r) for r in text_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25abc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying RegEx\n",
    "reviews_dates = re_dates(reviews_hiperlinks)\n",
    "df_comments['re_dates'] = reviews_dates\n",
    "\n",
    "# Verifying results\n",
    "print_step_result(reviews_hiperlinks, reviews_dates, idx_list=[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad9ffee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Special Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f488756b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def re_special_chars(text_list):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    ----------\n",
    "    text_series: list object with text content to be prepared [type: list]\n",
    "    \"\"\"\n",
    "    \n",
    "    # Applying regex\n",
    "    return [re.sub('\\W', ' ', r) for r in text_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1c4081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying RegEx\n",
    "reviews_special_chars = re_special_chars(reviews_dates)\n",
    "df_comments['re_special_chars'] = reviews_special_chars\n",
    "\n",
    "# Verifying results\n",
    "print_step_result(reviews_dates, reviews_special_chars, idx_list=[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b03af7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def re_whitespaces(text_list):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    ----------\n",
    "    text_series: list object with text content to be prepared [type: list]\n",
    "    \"\"\"\n",
    "    \n",
    "    # Applying regex\n",
    "    white_spaces = [re.sub('\\s+', ' ', r) for r in text_list]\n",
    "    white_spaces_end = [re.sub('[ \\t]+$', '', r) for r in white_spaces]\n",
    "    return white_spaces_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c880ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying RegEx\n",
    "reviews_whitespaces = re_whitespaces(reviews_special_chars)\n",
    "df_comments['re_whitespaces'] = reviews_whitespaces\n",
    "\n",
    "# Verifying results\n",
    "print_step_result(reviews_special_chars, reviews_whitespaces, idx_list=[3, 4, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018d7436",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7fc764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples of some english stopwords\n",
    "pt_stopwords = stopwords.words('english')\n",
    "print(f'Total english stopwords in the nltk.corpous module: {len(pt_stopwords)}')\n",
    "pt_stopwords[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba10314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to remove the stopwords and to lower the comments\n",
    "def stopwords_removal(text, cached_stopwords=stopwords.words('english')):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    ----------\n",
    "    text: list object where the stopwords will be removed [type: list]\n",
    "    cached_stopwords: stopwords to be applied on the process [type: list, default: stopwords.words('portuguese')]\n",
    "    \"\"\"\n",
    "    \n",
    "    return [c.lower() for c in text.split() if c.lower() not in cached_stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35a65de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing stopwords and looking at some examples\n",
    "reviews_stopwords = [' '.join(stopwords_removal(review)) for review in reviews_whitespaces]\n",
    "df_comments['stopwords_removed'] = reviews_stopwords\n",
    "\n",
    "print_step_result(reviews_whitespaces, reviews_stopwords, idx_list=[0, 45])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db89abc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "snowBallStemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45d87bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to remove the stopwords and to lower the comments\n",
    "def stemming_process(text, stemmer=SnowballStemmer(\"english\")):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    ----------\n",
    "    text: list object where the stopwords will be removed [type: list]\n",
    "    stemmer: type of stemmer to be applied [type: class, default: RSLPStemmer()]\n",
    "    \"\"\"\n",
    "    \n",
    "    return [stemmer.stem(c) for c in text.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3be23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying stemming and looking at some examples\n",
    "reviews_stemmer = [' '.join(stemming_process(review)) for review in reviews_stopwords]\n",
    "df_comments['stemming'] = reviews_stemmer\n",
    "\n",
    "print_step_result(reviews_stopwords, reviews_stemmer, idx_list=[0, 45, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6779eb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_from_corpus(corpus, vectorizer, df=False):\n",
    "    \"\"\"\n",
    "    Args\n",
    "    ------------\n",
    "    text: text to be transformed into a document-term matrix [type: string]\n",
    "    vectorizer: engine to be used in the transformation [type: object]\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extracting features\n",
    "    corpus_features = vectorizer.fit_transform(corpus).toarray()\n",
    "    features_names = vectorizer.get_feature_names()\n",
    "    \n",
    "    # Transforming into a dataframe to give interpetability to the process\n",
    "    df_corpus_features = None\n",
    "    if df:\n",
    "        df_corpus_features = pd.DataFrame(corpus_features, columns=features_names)\n",
    "    \n",
    "    return corpus_features, df_corpus_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95bab94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an object for the CountVectorizer class\n",
    "count_vectorizer = CountVectorizer(max_features=300, min_df=2, max_df=0.8, stop_words=pt_stopwords)\n",
    "\n",
    "# Extracting features for the corpus\n",
    "countv_features, df_countv_features = extract_features_from_corpus(reviews_stemmer, count_vectorizer, df=True)\n",
    "print(f'Shape of countv_features matrix: {countv_features.shape}\\n')\n",
    "print(f'Example of DataFrame of corpus features:')\n",
    "df_countv_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b71f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an object for the CountVectorizer class\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=300, min_df=2, max_df=0.8, stop_words=pt_stopwords)\n",
    "\n",
    "# Extracting features for the corpus\n",
    "tfidf_features, df_tfidf_features = extract_features_from_corpus(reviews_stemmer, tfidf_vectorizer, df=True)\n",
    "print(f'Shape of tfidf_features matrix: {tfidf_features.shape}\\n')\n",
    "print(f'Example of DataFrame of corpus features:')\n",
    "df_tfidf_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4709b438",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig, ax = plt.subplots(figsize=(10, 5))\n",
    "import seaborn as sns\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "sns.countplot(x='score', data=df_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a726507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labelling data\n",
    "score_map = {\n",
    "    1: 'negative',\n",
    "    2: 'negative',\n",
    "    3: 'negative',\n",
    "    4: 'positive',\n",
    "    5: 'positive'\n",
    "}\n",
    "df_comments['sentiment_label'] = df_comments['score'].map(score_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094a5e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = df_comments.groupby('sentiment_label')\n",
    "print(sent.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2fc500",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07437f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "# Create a function to get the subjectivity\n",
    "def getSubjectivity(text):\n",
    "    return TextBlob(text).sentiment.subjectivity\n",
    "\n",
    "# Create a function to get the polarity\n",
    "def getPolarity(text):\n",
    "    return  TextBlob(text).sentiment.polarity\n",
    "\n",
    "\n",
    "# Create two new columns 'Subjectivity' & 'Polarity'\n",
    "df_comments['Subjectivity'] = df_comments['stopwords_removed'].apply(getSubjectivity)\n",
    "df_comments['Polarity'] = df_comments['stopwords_removed'].apply(getPolarity)\n",
    "\n",
    "# Show the new dataframe with columns 'Subjectivity' & 'Polarity'\n",
    "df_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2386d869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to compute negative (-1), neutral (0) and positive (+1) analysis\n",
    "def getAnalysis(score):\n",
    "    if score < 0:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'positive'\n",
    "df_comments['sentiment_label'] = df_comments['Polarity'].apply(getAnalysis)\n",
    "# Show the dataframe\n",
    "df_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d750c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = df_comments.groupby('sentiment_label')\n",
    "print(sent.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c515920",
   "metadata": {},
   "source": [
    "# N-grams Plotly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85223ca",
   "metadata": {},
   "source": [
    "# Uni-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd4ca42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_words(corpus, n=None):\n",
    "    vec = CountVectorizer(stop_words='english').fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]\n",
    "common_words = get_top_n_words(df_comments['stopwords_removed'], 30)\n",
    "df2 = pd.DataFrame(common_words, columns = ['unigram' , 'count'])\n",
    "\n",
    "fig = go.Figure([go.Bar(x=df2['unigram'], y=df2['count'])])\n",
    "fig.update_layout(title=go.layout.Title(text=\"Top 30 unigrams in the Reviews\"))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b62c222",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_csv('unigram.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045d95ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680631a2",
   "metadata": {},
   "source": [
    "# Bi-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86affed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_bigram(corpus, n=None):\n",
    "    vec = CountVectorizer(ngram_range=(2, 2), stop_words='english').fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]\n",
    "common_words = get_top_n_bigram(df_comments['stopwords_removed'], 20)\n",
    "df3 = pd.DataFrame(common_words, columns = ['bigram' , 'count'])\n",
    "\n",
    "fig = go.Figure([go.Bar(x=df3['bigram'], y=df3['count'])])\n",
    "fig.update_layout(title=go.layout.Title(text=\"Top 20 bigrams in the Reviews\"))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ba2ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62a27df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.to_csv(\"bigrams.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a49cd74",
   "metadata": {},
   "source": [
    "# Tri-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4adbe0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_trigram(corpus, n=None):\n",
    "    vec = CountVectorizer(ngram_range=(3, 3), stop_words='english').fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]\n",
    "common_words = get_top_n_trigram(df_comments['stopwords_removed'], 20)\n",
    "df4 = pd.DataFrame(common_words, columns = ['trigram' , 'count'])\n",
    "\n",
    "fig = go.Figure([go.Bar(x=df4['trigram'], y=df4['count'])])\n",
    "fig.update_layout(title=go.layout.Title(text=\"Top 20 trigrams in the Reviews\"))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf0caab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bddf75",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.to_csv(\"trigrams.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37ab8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabb394a",
   "metadata": {},
   "source": [
    "# Word Cloud General (includes for all comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fca002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# background_color = https://www.google.com/search?q=hex+color+picker&oq=hex+color+&aqs=chrome.2.69i57j35i39j0i20i263i433i512j0i512l3j69i60l2.5061j0j7&sourceid=chrome&ie=UTF-8\n",
    "# colormap = https://matplotlib.org/stable/tutorials/colors/colormaps.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98810403",
   "metadata": {},
   "source": [
    "The description of the following arguments is below\n",
    "\n",
    "width/height: we can change the dimension of the canvas using these arguments. Here we assign width as 3000 and height as 2000.\n",
    "    \n",
    "random_state:  It will return PIL color for each word, set as an int value. \n",
    "    \n",
    "background_color: It is used for the background color of the word cloud image. \n",
    "    \n",
    "colormap: using this argument we can change each word color. Matplotlib colormaps provide awesome colors.\n",
    "    \n",
    "collocation: collocation argument is set to FALSE to ensure that the word cloud doesn’t contain any bigrams or duplicate words.\n",
    "    \n",
    "stopwords: ‘stop_words’ are those words that are commonly used in the English language such as ‘we’, ‘the’, ‘a’, ‘an’, etc. thus, we have to eliminate those words. we already imported the STOPWORDS function from the WordCloud library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf46aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8ad8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import package\n",
    "import matplotlib.pyplot as plt\n",
    "# Define a function to plot word cloud\n",
    "def plot_cloud(wordcloud):\n",
    "    # Set figure size\n",
    "    plt.figure(figsize=(40, 30))\n",
    "    # Display image\n",
    "    plt.imshow(wordcloud) \n",
    "    # No axis details\n",
    "    plt.axis(\"off\");\n",
    "\n",
    "# Generate wordcloud\n",
    "allWords = ' '.join([twts for twts in df_comments['stopwords_removed']])\n",
    "wordcloud = WordCloud(width = 3000, height = 2000, random_state=1,background_color='black', colormap='Set2', collocations=False, stopwords = STOPWORDS).generate(allWords)\n",
    "#wordcloud = WordCloud(width = 3000, height = 2000, random_state=1, background_color='black', colormap='Set2', collocations=False, stopwords = STOPWORDS).generate(text)\n",
    "#wordcloud = WordCloud(width = 3000, height = 2000, random_state=1,max_words=50,background_color='black', colormap='Set2', collocations=False, stopwords = STOPWORDS).generate(allWords)\n",
    "# Plot\n",
    "plot_cloud(wordcloud)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49287461",
   "metadata": {},
   "source": [
    "# Positive N-grams Plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b42fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_words(corpus, n=None):\n",
    "    vec = CountVectorizer(stop_words='english').fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]\n",
    "\n",
    "df_positive=df_comments[df_comments[\"sentiment_label\"] == \"positive\"]\n",
    "common_words = get_top_n_words(df_positive['stopwords_removed'], 30)\n",
    "dfp1 = pd.DataFrame(common_words, columns = ['unigram' , 'count'])\n",
    "\n",
    "fig = go.Figure([go.Bar(x=dfp1['unigram'], y=dfp1['count'])])\n",
    "fig.update_layout(title=go.layout.Title(text=\"Top 30 unigrams in the Reviews\"))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562cd193",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfp1.to_csv(\"positive_unigrams.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10d23cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_bigram(corpus, n=None):\n",
    "    vec = CountVectorizer(ngram_range=(2, 2), stop_words='english').fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]\n",
    "df_positive=df_comments[df_comments[\"sentiment_label\"] == \"positive\"]\n",
    "common_words = get_top_n_bigram(df_positive['stopwords_removed'], 20)\n",
    "dfp2 = pd.DataFrame(common_words, columns = ['bigram' , 'count'])\n",
    "\n",
    "\n",
    "fig = go.Figure([go.Bar(x=dfp2['bigram'], y=dfp2['count'])])\n",
    "fig.update_layout(title=go.layout.Title(text=\"Top 20 bigrams in the Reviews\"))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a23b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfp2.to_csv(\"positive_bigrams.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90636885",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_trigram(corpus, n=None):\n",
    "    vec = CountVectorizer(ngram_range=(3, 3), stop_words='english').fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]\n",
    "df_positive=df_comments[df_comments[\"sentiment_label\"] == \"positive\"]\n",
    "common_words = get_top_n_trigram(df_positive['stopwords_removed'], 20)\n",
    "dfp4 = pd.DataFrame(common_words, columns = ['trigram' , 'count'])\n",
    "\n",
    "fig = go.Figure([go.Bar(x=dfp4['trigram'], y=dfp4['count'])])\n",
    "fig.update_layout(title=go.layout.Title(text=\"Top 20 trigrams in the Reviews\"))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932f0b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfp4.to_csv(\"positive_trigrams.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b22a6d6",
   "metadata": {},
   "source": [
    "# POSITIVE WORD CLOUD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4329d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import package\n",
    "import matplotlib.pyplot as plt\n",
    "# Define a function to plot word cloud\n",
    "def plot_cloud(wordcloud):\n",
    "    # Set figure size\n",
    "    plt.figure(figsize=(40, 30))\n",
    "    # Display image\n",
    "    plt.imshow(wordcloud) \n",
    "    # No axis details\n",
    "    plt.axis(\"off\");\n",
    "\n",
    "# Generate wordcloud\n",
    "df_positive=df_comments[df_comments[\"sentiment_label\"] == \"positive\"]\n",
    "#df_positive=df3[df3[\"bigram\"]]\n",
    "allWords = ' '.join([twts for twts in df_positive['stopwords_removed']])\n",
    "wordcloud = WordCloud(width = 3000, height = 2000, random_state=1, background_color='black', colormap='Set2', collocations=False, stopwords = STOPWORDS).generate(allWords)\n",
    "# Plot\n",
    "plot_cloud(wordcloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3a2953",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_positive.to_csv(\"positive_reviews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11656d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5748aa2f",
   "metadata": {},
   "source": [
    "# Negative n-grams Plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6bee70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_words(corpus, n=None):\n",
    "    vec = CountVectorizer(stop_words='english').fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]\n",
    "df_positive=df_comments[df_comments[\"sentiment_label\"] == \"negative\"]\n",
    "common_words = get_top_n_words(df_positive['stopwords_removed'], 30)\n",
    "dfn1 = pd.DataFrame(common_words, columns = ['unigram' , 'count'])\n",
    "\n",
    "fig = go.Figure([go.Bar(x=dfn1['unigram'], y=dfn1['count'])])\n",
    "fig.update_layout(title=go.layout.Title(text=\"Top 30 unigrams in the Reviews\"))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f8552a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfn1.to_csv(\"negative_unigrams.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb60701c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_bigram(corpus, n=None):\n",
    "    vec = CountVectorizer(ngram_range=(2, 2), stop_words='english').fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]\n",
    "df_positive=df_comments[df_comments[\"sentiment_label\"] == \"negative\"]\n",
    "common_words = get_top_n_bigram(df_positive['stopwords_removed'], 20)\n",
    "dfn2 = pd.DataFrame(common_words, columns = ['bigram' , 'count'])\n",
    "\n",
    "fig = go.Figure([go.Bar(x=dfn2['bigram'], y=dfn2['count'])])\n",
    "fig.update_layout(title=go.layout.Title(text=\"Top 20 bigrams in the Reviews\"))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d872e724",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfn2.to_csv(\"negative_bigrams.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c1a2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_trigram(corpus, n=None):\n",
    "    vec = CountVectorizer(ngram_range=(3, 3), stop_words='english').fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]\n",
    "df_positive=df_comments[df_comments[\"sentiment_label\"] == \"negative\"]\n",
    "common_words = get_top_n_trigram(df_positive['stopwords_removed'], 20)\n",
    "dfn3 = pd.DataFrame(common_words, columns = ['trigram' , 'count'])\n",
    "\n",
    "fig = go.Figure([go.Bar(x=dfn3['trigram'], y=dfn3['count'])])\n",
    "fig.update_layout(title=go.layout.Title(text=\"Top 20 trigrams in the Reviews\"))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa1603a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfn3.to_csv(\"negative_trigrams.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a802c8",
   "metadata": {},
   "source": [
    "# NEGATIVE WORD CLOUD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1baf1ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import package\n",
    "import matplotlib.pyplot as plt\n",
    "# Define a function to plot word cloud\n",
    "def plot_cloud(wordcloud):\n",
    "    # Set figure size\n",
    "    plt.figure(figsize=(40, 30))\n",
    "    # Display image\n",
    "    plt.imshow(wordcloud) \n",
    "    # No axis details\n",
    "    plt.axis(\"off\");\n",
    "\n",
    "# Generate wordcloud\n",
    "df_negative=df_comments[df_comments[\"sentiment_label\"] == \"negative\"]\n",
    "allWords = ' '.join([twts for twts in df_negative['stopwords_removed']])\n",
    "wordcloud = WordCloud(width = 3000, height = 2000, random_state=1, background_color='black', colormap='Set2', collocations=False, stopwords = STOPWORDS).generate(allWords)\n",
    "# Plot\n",
    "plot_cloud(wordcloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b954814e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
